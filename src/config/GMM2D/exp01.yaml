exp_title: original
dataset_name: GMM2D
train_classes: ["0", "1"]
num_per_class: [1000, 1000]
classification_model: Logistic_Regression_2D
influence_model: hashmap_IF
max_epoch: 400
influence_lr: 0.001
influence_momentum: 0.9
influence_weight_decay: 0.
optimizer_influence: SGD
classification_lr: 0.001
classification_momentum: 0.9
classification_weight_decay: 0.01
optimizer_classification: SGD
batch_size: 64
_hidden_size_classification: 32
_hidden_size_influence: 32
dataWeight_weight_decay: 1 # 1 means no weight decay equation: w / decay
dataWeight_weight_init: 0.
softmax_temp: 10
train_classification_till_converge: False
use_pretrain_classification: True
reset_pretrain_classification_every_epoch: False
clip_min_weight: True
dev_id_num: 1
dev_original_folder: data/MNIST/dev/oneClassEach/original_sampled
dev_transformed_folder: data/MNIST/dev/oneClassEach/transformed_sampled #useless now
test_original_folder: "" #useless now
test_transformed_folder: "" #useless now
_gpu_id: 6

seed: 1
max_pretrain_epoch: 1600
max_checkpoint_epoch: 1600
pretrain_classification_lr: 0.001 #use 0.001 lr and 100epoch for pretrain default
_pretrain_ckpt_name: classAll1000Each.pt #specify the dataset used for training

normalize_fn_classification: softmax
normalize_fn_influence: linear
_num_class: 2 # the maximum number of class
_ckpt_name: last

compare_with_inv_hessian: True
compare_with_identity: True

